h1. GitHub Contest: August 2009

"github-contest":http://github.com/jbrownlee/github-contest submission by jbrownlee.

For background on the contest, see the "github contest website":http://contest.github.com 

h2. Author

"Jason Brownlee":mailto:jason.brownlee05@gmai.com

h2. Description

This project provides code to generate a solution (results.txt) to the "github contest":http://contest.github.com August 2009.

<b>Note</b>: This is a project designed for me to learn more about Ruby rather than seriously compete in the competition.

h2. Details

The objective is to recommend repositories to users. Specifically to predict which repositories to recommend to a user given existing known user-repository relationships.

h3. Data

The following is a summary of the data provided in the competition, see /data/readme.txt for more detail.

* <b>data.txt</b> 440,237 known user-to-repository relationships
* <b>repos.txt</b> 120,867 known repositories (id, name, date created, parent repository)
* <b>lang.txt</b> language information for 73,496 repositories (id,lang;total_lines,...)
* <b>test.txt</b> 4,788 users from data.txt, each of which requires a prediction of (up to) 10 repositories

Some indicators that may be exploited include
* language correlation
* repository size correlation
* repository hierarchy size or type correlation

h3. Strategies

* Random: This is my base line strategy. It provides proof of concept for data parsing, internal testing and submission. For each user id a random set between 0 and 10 of existing repositories are recommended.

h3. Validation Testing

The internet (pre-submission) testing strategy involves a 10-fold cross-validation on the provided data.text relationships. This is where the training data is partitioned (by user id) into 10 distinct sections. A model is then created from 9 parts and tested on the 10th. This process is repeated 9 more times for each part and the scoring averaged. 

The scoring for a single user is calculated by assigning one point to each correct prediction for each user without a penalty for incorrect guesses and without considering recommendation rank. The scoring for one run is the percentage of correct recommendations. 

h3. Open Questions

# How exactly does the online scoring work? 
## Are incorrect recommendations punished?
## Is recommendation order important (list or set of recommendations)?

h2. Links and References

# "Github contest page":http://contest.github.com
# "Github contest leaderboard":http://contest.github.com/leaderboard
# "Sample submission by schacon":http://github.com/schacon/test-entry

h2. License 

(The MIT License) 

Copyright (c) 2009 Jason Brownlee

Permission is hereby granted, free of charge, to any person
obtaining a copy of this software and associated documentation
files (the "Software"), to deal in the Software without
restriction, including without limitation the rights to use,
copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the
Software is furnished to do so, subject to the following
conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.